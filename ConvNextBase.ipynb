{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "The people of Malawi have experienced an array of natural disasters and climatic shocks in recent years, including droughts, floods and landslides. The economic impacts of these disasters, coupled with Covid-19 and other global issues, negatively affect the health and wellbeing of most Malawians. People living in rural areas (more than 80% of Malawians) have been hit hardest.\n",
    "\n",
    "There have been great strides in mapping flood extents and corresponding damages caused by these floods around the world, using satellite imagery. However, there are still gaps in determining the real number of affected populations, especially in rural areas in Malawi. Many houses in rural areas are often constructed with traditional grass-thatched roofs, and these are missed by the algorithms using satellite or aerial imagery to count populations or identify buildings affected by floods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "The objective of this competition is to create a machine-learning algorithm that counts the number of grass-thatch, tin and other roofed houses in aerial (drone) imagery. Ensuring more accurate estimates of affected populations in the case of a disaster allows these communities to be evacuated or for aid to be provided more effectively, helping to improve response times and save lives in rural Malawi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding environment\n",
    "This Convnext notebook was trained with NVIDIA RTX A6000 and it takes approximately 10 hours to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Data preprocessing, particularly oversampling based on error analysis, became pivotal due to significant class imbalances within our dataset. To address this challenge, we applied several techniques aimed at augmenting the minority classes. These techniques helped increase the representation of underrepresented classes, thereby enhancing the model's ability to generalize across all categories. Prior to this, we filtered the background images and retained only 10% for training. In this notebook, the augmentation techniques we utilized include:\n",
    "\n",
    "* **Copy-Paste:**\n",
    "The copy-paste technique involves inserting foreground objects or elements from one image (such as annotated objects) onto another background image. This method effectively diversifies training data by combining annotated objects with various background scenes. By leveraging this technique, we aimed to enhance the dataset's diversity and ensure that the model learns to recognize objects in different contextual settings.\n",
    "\n",
    "* **Random Image Masking:**\n",
    "Random image masking involves applying random geometric shapes or patterns to obscure parts of an image. This technique serves multiple purposes, including data augmentation and regularization. By randomly masking regions of the image with shapes like polygons, circles, triangles, or rectangles, we aimed to simulate occlusions or varying environmental conditions. This approach encourages the model to focus on relevant features and improves its robustness to noise and partial occlusions in real-world scenarios.\n",
    "\n",
    "* **Masked Scale:**\n",
    "Masked scale involves resizing and masking images to simulate various scales and viewpoints of objects within the image. Each chosen image is resized to 768x768 pixels and then placed randomly within a 1000x1000 pixel canvas, with additional random rotations of up to Â±45 degrees. This technique augments the dataset by generating variations in object sizes and positions relative to the image frame. Scaling and masking images aim to enhance the model's capability to generalize across different object sizes and orientations by exposing it to diverse perspectives and scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers==4.38.2 timm==0.9.16 pytorch_lightning==2.2.1 datasets==2.3.2 torchmetrics==1.3.2 albumentations==1.0.3 scikit-learn==1.2.2 imutils==0.5.4 shapely==2.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import imutils\n",
    "import albumentations as A\n",
    "from random import randint, choice, choices\n",
    "import random\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import pathlib\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import GroupKFold, train_test_split, StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score , balanced_accuracy_score, log_loss\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import albumentations\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torchmetrics\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "def split_function(\n",
    "    df,\n",
    "    n_splits,\n",
    "    method=\"groupkfold\",\n",
    "    target_col=\"\",\n",
    "    unique_id=\"\"\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into folds for cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to split.\n",
    "    - n_splits (int): Number of folds.\n",
    "    - method (str): The method to use for splitting. Options are 'groupkfold' and 'stratifiedgroupkfold'.\n",
    "    - target_col (str): The name of the target column.\n",
    "    - unique_id (str): The name of the column with unique group identifiers.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The input DataFrame with an added 'fold' column indicating the fold assignment.\n",
    "    \"\"\"\n",
    "    set_seed(seed=42)\n",
    "    df = df.copy()\n",
    "    if method == \"groupkfold\":\n",
    "        folds = GroupKFold(n_splits=n_splits)\n",
    "    elif method == \"straitifiedgroupkfold\":\n",
    "        folds = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    df['fold'] = -1\n",
    "    for i,(train_index, test_index) in enumerate(\n",
    "        folds.split(\n",
    "            df,\n",
    "            df[target_col], \n",
    "            groups=df[unique_id]\n",
    "        )\n",
    "    ): \n",
    "        df.loc[test_index,'fold'] = i\n",
    "    return df\n",
    "\n",
    "def create_background_df(\n",
    "    df,\n",
    "    downsample_factor = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a background DataFrame by filtering and downsampling the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing image data.\n",
    "    - downsample_factor (float): The fraction of the background data to sample. Default is 0.2.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A downsampled DataFrame containing background images with label columns set to 0.\n",
    "    \"\"\"\n",
    "    background = df[df['bbox'].isna()][['image_id']]\n",
    "    for col in CFG.data_preparation.labels: \n",
    "        background[col] = 0\n",
    "\n",
    "    background = background.sample(frac=downsample_factor, random_state=CFG.seed)\n",
    "    return background\n",
    "\n",
    "\n",
    "def create_target_df(df):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for training by processing image paths, extracting image dimensions,\n",
    "    splitting into folds, and adding background images.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing image data and annotations.\n",
    "    \n",
    "    Returns:\n",
    "    - original_df (pd.DataFrame): The original DataFrame with added image dimensions and processed bounding boxes.\n",
    "    - result_df (pd.DataFrame): The DataFrame ready for training, including fold information and background images.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # create_path_column\n",
    "    df['path'] = df['image_id'].apply(\n",
    "        lambda id_ : os.path.join(\n",
    "            CFG.data_preparation.images_path,\n",
    "            f\"{id_}.tif\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Assuming 'all_paths' is a list of all unique image paths from train set\n",
    "    all_paths = df.path.unique().tolist()\n",
    "\n",
    "    # Use joblib to process paths in parallel and get results\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_path)(path) for path in tqdm(all_paths)\n",
    "    )\n",
    "\n",
    "    # Initialize dictionaries to store widths and heights\n",
    "    width_mapper = {}\n",
    "    height_mapper = {}\n",
    "\n",
    "    # Populate the dictionaries with the results\n",
    "    for path, width, height in results:\n",
    "        width_mapper[path] = width\n",
    "        height_mapper[path] = height\n",
    "\n",
    "    # create width, height columns (to be used in yolo conversion)\n",
    "    df['width'] = df['path'].map(width_mapper)\n",
    "    df['height'] = df['path'].map(height_mapper)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # will be used later for oversampling\n",
    "    original_df = df.copy()\n",
    "    \n",
    "    # split function\n",
    "    split_df = split_function(\n",
    "        df,\n",
    "        n_splits=CFG.data_preparation.n_splits,\n",
    "        unique_id=CFG.data_preparation.group_col,\n",
    "        target_col=CFG.data_preparation.split_col,\n",
    "    )\n",
    "    \n",
    "    # get background images\n",
    "    background = create_background_df(\n",
    "        df,\n",
    "        downsample_factor=CFG.data_preparation.downsample_factor\n",
    "    )\n",
    "    \n",
    "    df = df[df['category_id'].isin(CFG.data_preparation.targets)].reset_index(drop=True)\n",
    "    \n",
    "    category_counts = df.groupby(\n",
    "        ['image_id', 'category_id']\n",
    "    ).size().reset_index(name='count')\n",
    "    \n",
    "    result_df = category_counts.pivot_table(\n",
    "        index='image_id',\n",
    "        columns='category_id',\n",
    "        values='count',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    result_df.columns = ['image_id'] + CFG.data_preparation.labels\n",
    "    \n",
    "    for col in CFG.data_preparation.labels:\n",
    "        result_df[col] = result_df[col].astype(int)\n",
    "        \n",
    "    result_df=pd.concat([result_df, background], axis = 0).reset_index(drop=True)\n",
    "    result_df['count'] = result_df[CFG.data_preparation.labels].sum(axis=1)\n",
    "    result_df = pd.merge(result_df, split_df[['image_id','fold']], on='image_id', how='left').drop_duplicates()\n",
    "    result_df['path'] = result_df['image_id'].apply(\n",
    "        lambda id_ : os.path.join(\n",
    "            CFG.data_preparation.images_path,\n",
    "            f\"{id_}.tif\"\n",
    "        )\n",
    "    )\n",
    "    return original_df, result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_path(path):\n",
    "    \"\"\"\n",
    "    Reads an image and returns its path along with its width and height.\n",
    "    Parameters:\n",
    "    - path: str, the path to the image file.\n",
    "    Returns:\n",
    "    - tuple: (path, width, height)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    height, width, _ = img.shape  # Get image dimensions\n",
    "    return (path, width, height)\n",
    "\n",
    "\n",
    "# filter bad background data\n",
    "def bad_background(path):\n",
    "    \"\"\"\n",
    "    Determines if an image has a bad background based on its mean pixel values.\n",
    "    \n",
    "    Parameters:\n",
    "    - path (str): The file path of the image.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the background is bad, False otherwise.\n",
    "    \"\"\"\n",
    "    bad_background = False\n",
    "    image = cv2.imread(path)\n",
    "    mean_values = image.mean()\n",
    "    if (mean_values<=10) or (mean_values>=240):\n",
    "        bad_background = True\n",
    "    return bad_background\n",
    "    \n",
    "    \n",
    "def decode_points(ddbox: str) -> list:\n",
    "    \"\"\"\n",
    "    Decodes a bounding box string into a list of integers.\n",
    "    \n",
    "    Parameters:\n",
    "    - ddbox (str): The bounding box string to decode.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of integers representing the bounding box coordinates.\n",
    "    \"\"\"\n",
    "    points = [int(float(point)) for point in re.findall(r'\\d+\\.\\d+', ddbox)]\n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "def can_place_object(new_bbox, existing_bboxes, background_image):\n",
    "    \"\"\"\n",
    "    Checks if a new bounding box can be placed on an image without overlapping existing bounding boxes\n",
    "    and ensures the region has an appropriate intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - new_bbox (list): The new bounding box coordinates [x, y, width, height].\n",
    "    - existing_bboxes (list): A list of existing bounding boxes.\n",
    "    - background_image (ndarray): The image on which the bounding box will be placed.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the new bounding box can be placed, False otherwise.\n",
    "    \"\"\"\n",
    "    for bbox in existing_bboxes:\n",
    "        if (new_bbox[0] < bbox[0] + bbox[2] and new_bbox[0] + new_bbox[2] > bbox[0] and\n",
    "            new_bbox[1] < bbox[1] + bbox[3] and new_bbox[1] + new_bbox[3] > bbox[1]):\n",
    "            return False\n",
    "    \n",
    "    roi = background_image[new_bbox[1]:new_bbox[1]+new_bbox[3], new_bbox[0]:new_bbox[0]+new_bbox[2]]\n",
    "    avg_intensity = np.mean(roi)\n",
    "    if avg_intensity > 200 or avg_intensity < 50:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process a single background image\n",
    "def process_background_image(\n",
    "    bg_row,\n",
    "    labels_data,\n",
    "    folder_path,\n",
    "    seed,\n",
    "    idx\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a single background image by copy-pasting objects from the annotated data.\n",
    "\n",
    "    Parameters:\n",
    "    - bg_row (pd.Series): A row from the background data DataFrame.\n",
    "    - labels_data (pd.DataFrame): DataFrame containing annotated objects and their metadata.\n",
    "    - folder_path (str): Path to the folder where processed images will be saved.\n",
    "    - seed (int): Seed for random number generation to ensure reproducibility.\n",
    "    - idx (int): Index of the current background image.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing metadata of the new images with pasted objects.\n",
    "    \"\"\"\n",
    "    set_seed(seed=seed+idx+1)\n",
    "    tmp_data = pd.DataFrame(columns=labels_data.columns)\n",
    "    n_obj_candidates = random.randint(2, 100)\n",
    "    background_image = cv2.imread(bg_row.path)\n",
    "    bg_height, bg_width, _ = background_image.shape\n",
    "    \n",
    "    sampled_annotated_data = labels_data[\n",
    "        labels_data['height'] == bg_height\n",
    "    ].sample(\n",
    "        frac=1.0,\n",
    "        random_state=n_obj_candidates\n",
    "    ).sample(\n",
    "        n=n_obj_candidates,\n",
    "        random_state=seed * n_obj_candidates\n",
    "    )\n",
    "    \n",
    "    copy_pasted_bboxes = []\n",
    "    images_id = []\n",
    "    images_path = []\n",
    "    images_width = []\n",
    "    images_height = []\n",
    "    obj_categories = []\n",
    "    save_path = os.path.join(\n",
    "        folder_path,\n",
    "        f\"{bg_row.image_id}.tif\"\n",
    "    )\n",
    "\n",
    "    for annot_idx, annot_row in sampled_annotated_data.iterrows():\n",
    "        object_bbox = decode_points(annot_row.bbox)\n",
    "        category_id = annot_row.category_id\n",
    "        annot_image = cv2.imread(annot_row.path)\n",
    "\n",
    "        if can_place_object(object_bbox, copy_pasted_bboxes,background_image):\n",
    "            background_image[\n",
    "                object_bbox[1]:object_bbox[1] + object_bbox[3],\n",
    "                object_bbox[0]:object_bbox[0] + object_bbox[2]\n",
    "            ] = annot_image[\n",
    "                object_bbox[1]:object_bbox[1] + object_bbox[3],\n",
    "                object_bbox[0]:object_bbox[0] + object_bbox[2]\n",
    "            ]\n",
    "            copy_pasted_bboxes.append(object_bbox)\n",
    "            images_id.append(bg_row.image_id)\n",
    "            images_path.append(save_path)\n",
    "            images_width.append(bg_width)\n",
    "            images_height.append(bg_height)\n",
    "            obj_categories.append(category_id)\n",
    "\n",
    "    cv2.imwrite(save_path, background_image)\n",
    "    \n",
    "    tmp_data['image_id'] = images_id\n",
    "    tmp_data['width'] = images_width\n",
    "    tmp_data['height'] = images_height\n",
    "    tmp_data['path'] = images_path\n",
    "    tmp_data['category_id'] = obj_categories\n",
    "    tmp_data['bbox'] = copy_pasted_bboxes\n",
    "    return tmp_data\n",
    "\n",
    "def copy_paste(\n",
    "    labels_data,\n",
    "    background_data,\n",
    "    folder_path,\n",
    "    seed\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs copy-pasting of objects onto background images in parallel.\n",
    "\n",
    "    Parameters:\n",
    "    - labels_data (pd.DataFrame): DataFrame containing annotated objects and their metadata.\n",
    "    - background_data (pd.DataFrame): DataFrame containing background images metadata.\n",
    "    - folder_path (str): Path to the folder where processed images will be saved.\n",
    "    - seed (int): Seed for random number generation to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing metadata of the new images with pasted objects.\n",
    "    \"\"\"\n",
    "    # Initialize new DataFrame for results\n",
    "    new_data = pd.DataFrame(columns=labels_data.columns)\n",
    "    # Run processing in parallel\n",
    "    results = Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "        delayed(process_background_image)(\n",
    "            bg_row,\n",
    "            labels_data,\n",
    "            folder_path,\n",
    "            seed,\n",
    "            idx,\n",
    "        )\n",
    "        for idx, bg_row in tqdm(\n",
    "            background_data.iterrows(),\n",
    "            total=len(background_data)\n",
    "        )\n",
    "    )\n",
    "    # Combine results into a single DataFrame\n",
    "    new_data = pd.concat(results).reset_index(drop=True)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_image_masking(image, boxes, y):\n",
    "    \"\"\"\n",
    "    Applies random masking to an image by drawing random shapes and then checks\n",
    "    if the bounding boxes are still valid based on the mean pixel value inside the box.\n",
    "\n",
    "    Parameters:\n",
    "    - image (ndarray): The input image.\n",
    "    - boxes (list): List of bounding boxes.\n",
    "    - y (list): List of labels corresponding to the bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The masked image, updated bounding boxes, and updated labels.\n",
    "    \"\"\"\n",
    "    mask_count = randint(1, 3)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Define a list of functions that generate different shapes\n",
    "    def draw_polygon():\n",
    "        points = np.array([(randint(0, width), randint(0, height)) for _ in range(4)], dtype=np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "        cv2.fillPoly(image, [points], (0, 0, 0))\n",
    "\n",
    "    def draw_circle():\n",
    "        center = (randint(0, width), randint(0, height))\n",
    "        radius = randint(10, min(height, width) // 4)\n",
    "        cv2.circle(image, center, radius, (0, 0, 0), thickness=-1)\n",
    "\n",
    "    def draw_triangle():\n",
    "        points = np.array([(randint(0, width), randint(0, height)) for _ in range(3)], dtype=np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "        cv2.fillPoly(image, [points], (0, 0, 0))\n",
    "\n",
    "    def draw_rectangle():\n",
    "        pt1 = (randint(0, width), randint(0, height))\n",
    "        pt2 = (randint(pt1[0], width), randint(pt1[1], height))\n",
    "        cv2.rectangle(image, pt1, pt2, (0, 0, 0), thickness=-1)\n",
    "\n",
    "    shape_functions = [draw_polygon, draw_circle, draw_triangle, draw_rectangle]\n",
    "\n",
    "    for _ in range(mask_count):\n",
    "        # Randomly select and draw a shape\n",
    "        shape_func = choice(shape_functions)\n",
    "        shape_func()\n",
    "\n",
    "    # Calculate if boxes are still valid\n",
    "    new_boxes = []\n",
    "    new_y = []\n",
    "    for box, label in zip(boxes, y):\n",
    "        box = ast.literal_eval(box)  # ensure box is in tuple/list form if coming from a string\n",
    "        box = list(map(int, box))\n",
    "        # Check mean pixel value in the box to decide if it should be kept\n",
    "        if np.mean(\n",
    "            image[\n",
    "                box[1]:box[3] + box[1],\n",
    "                box[0]:box[2] + box[0],\n",
    "            ]\n",
    "        ) > 5:  # Check mean pixel value in the box\n",
    "            new_boxes.append(box)\n",
    "            new_y.append(label)\n",
    "\n",
    "    return image, new_boxes, new_y\n",
    "\n",
    "\n",
    "def random_masking(\n",
    "    labels_data,\n",
    "    columns,\n",
    "    output_directory,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies random masking to images in a dataset and saves the augmented images.\n",
    "\n",
    "    Parameters:\n",
    "    - labels_data (pd.DataFrame): DataFrame containing image paths and bounding boxes.\n",
    "    - columns (list): List of columns for the temporary DataFrame.\n",
    "    - output_directory (str): Directory where augmented images will be saved.\n",
    "    - seed (int): Seed for random number generation.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing metadata of the new images with augmented objects.\n",
    "    \"\"\"\n",
    "    # initialize final data\n",
    "    final_data = pd.DataFrame()\n",
    "    \n",
    "    !rm -r $output_directory\n",
    "    os.makedirs(output_directory, exist_ok=False)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    n_candidates = randint(50, 200)\n",
    "    image_paths = labels_data.drop_duplicates('path').path.tolist()\n",
    "    selected_images = choices(image_paths, k=n_candidates)\n",
    "    \n",
    "    for idx, path in tqdm(enumerate(selected_images)):\n",
    "        tmp_data = pd.DataFrame(columns=columns)\n",
    "        subdf = labels_data[labels_data['path']==path]\n",
    "        image = cv2.imread(path)\n",
    "        boxes = subdf.bbox.tolist()\n",
    "        y = subdf.category_id.tolist()\n",
    "\n",
    "        new_image, updated_boxes, updated_y = random_image_masking(\n",
    "            image,\n",
    "            boxes,\n",
    "            y\n",
    "        )\n",
    "        \n",
    "        updated_path = os.path.join(\n",
    "            output_directory,\n",
    "            os.path.basename(path)\n",
    "        )\n",
    "        tmp_data['bbox'] = updated_boxes\n",
    "        tmp_data['category_id'] = updated_y\n",
    "        tmp_data['path'] = updated_path\n",
    "            \n",
    "        # Save the augmented image\n",
    "        cv2.imwrite(\n",
    "            updated_path,\n",
    "            new_image\n",
    "        )\n",
    "        \n",
    "        # update final data\n",
    "        final_data = pd.concat(\n",
    "            [\n",
    "                final_data,\n",
    "                tmp_data\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    # Count the occurrences of each category per image_id\n",
    "    category_counts = final_data.groupby(['path', 'category_id']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns as required\n",
    "    category_counts.columns = [f'category_{int(x-1)}' for x in category_counts.columns]\n",
    "    for col in ['category_0', 'category_1', 'category_2']:\n",
    "        if col not in category_counts.columns:\n",
    "            category_counts[col] = 0\n",
    "    # Merge this with the original DataFrame on 'image_id'\n",
    "    result_data = final_data.merge(category_counts.reset_index(), on='path', how='left').copy()\n",
    "    result_data['count'] = 0\n",
    "    result_data['fold'] = 0\n",
    "    result_data = result_data.drop_duplicates('path')\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_scale(\n",
    "    labels_data,\n",
    "    output_directory,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply random transformations to images, create masks, and save the augmented images.\n",
    "\n",
    "    Parameters:\n",
    "    - labels_data (pd.DataFrame): DataFrame containing image paths and bounding boxes.\n",
    "    - output_directory (str): Directory where augmented images will be saved.\n",
    "    - seed (int): Seed for random number generation.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing metadata of the new images with augmented objects.\n",
    "    \"\"\"\n",
    "    # initialize final data\n",
    "    final_data = pd.DataFrame()\n",
    "    \n",
    "    !rm -r $output_directory\n",
    "    os.makedirs(output_directory, exist_ok=False)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # algo inputs\n",
    "    labels_data = labels_data[labels_data['height']==500].copy()\n",
    "    other_images = labels_data[labels_data['category_id']==1].drop_duplicates('path').path.tolist()\n",
    "    thatch_images = labels_data[labels_data['category_id']==3].drop_duplicates('path').path.tolist()\n",
    "    selected_images = labels_data.drop_duplicates('path').sample(n=150, random_state=seed).path.tolist()\n",
    "    \n",
    "    # Ensuring at least 50 'Thatch' images if possible\n",
    "    selected_thatch_images = [img for img in selected_images if img in thatch_images]\n",
    "    selected_other_images = [img for img in selected_images if img in other_images]\n",
    "\n",
    "    if len(selected_thatch_images) < 50:\n",
    "        additional_thatch = [img for img in thatch_images if img not in selected_images]\n",
    "        selected_images += choices(additional_thatch, k=50 - len(selected_thatch_images))\n",
    "        \n",
    "    if len(selected_other_images) < len(other_images):\n",
    "        additional_other = [img for img in other_images if img not in selected_images]\n",
    "        selected_images += additional_other\n",
    "    \n",
    "    # Load images, create masks, apply transformations, and save\n",
    "    transform = A.Compose([\n",
    "        A.Resize(768, 768, p=0.75),\n",
    "        A.HorizontalFlip(p=1.0), \n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.RandomBrightnessContrast(p=1.0),\n",
    "    ])\n",
    "\n",
    "    for idx, image_path in tqdm(enumerate(selected_images)):\n",
    "        tmp_data = labels_data[labels_data['path']==image_path]\n",
    "        new_path = os.path.join(\n",
    "            output_directory,\n",
    "            os.path.basename(image_path)\n",
    "        )\n",
    "        tmp_data['path'] = new_path\n",
    "        random.seed(seed+1+idx)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Apply albumentations\n",
    "        image = transform(image=image)['image']\n",
    "        \n",
    "        # create empty mask\n",
    "        mask = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
    "        # Randomly place image in mask\n",
    "        x_offset = randint(0, 1000 - image.shape[1])\n",
    "        y_offset = randint(0, 1000 - image.shape[0])\n",
    "        mask[y_offset:y_offset+image.shape[0], x_offset:x_offset+image.shape[1]] = image\n",
    "        \n",
    "        # Apply rotation using imutils\n",
    "        angle = randint(-45, 45)\n",
    "        mask = imutils.rotate(mask, angle)\n",
    "        \n",
    "        # Save the augmented image\n",
    "        cv2.imwrite(\n",
    "            new_path,\n",
    "            mask\n",
    "        )\n",
    "        \n",
    "        # update final data\n",
    "        final_data = pd.concat(\n",
    "            [\n",
    "                final_data,\n",
    "                tmp_data\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    # Count the occurrences of each category per image_id\n",
    "    category_counts = final_data.groupby(['path', 'category_id']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns as required\n",
    "    category_counts.columns = [f'category_{int(x-1)}' for x in category_counts.columns]\n",
    "    for col in ['category_0', 'category_1', 'category_2']:\n",
    "        if col not in category_counts.columns:\n",
    "            category_counts[col] = 0\n",
    "    # Merge this with the original DataFrame on 'image_id'\n",
    "    result_data = final_data.merge(category_counts.reset_index(), on='path', how='left').copy()\n",
    "    result_data['count'] = 0\n",
    "    result_data['fold'] = 0\n",
    "    result_data = result_data.drop_duplicates('path')\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        config,\n",
    "        transform=None,\n",
    "        mode='training'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a DataFrame, configuration, transforms, and mode.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): DataFrame containing the dataset information.\n",
    "        - config (object): Configuration object with necessary parameters.\n",
    "        - transform (callable, optional): A function/transform to apply to the images.\n",
    "        - mode (str): Mode of the dataset, either 'training' or 'inference'.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.df =df\n",
    "        self.mode = mode\n",
    "        self.config = config\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and its corresponding target for a given index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - If mode is 'training': (image, target) tuple.\n",
    "        - If mode is 'inference': image only.\n",
    "        \"\"\"\n",
    "        row = self.df.loc[idx]\n",
    "        image = Image.open(row[self.config.training_params.image_path_col])\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            image = self.transform(\n",
    "                image=np.array(image)\n",
    "            )['image']\n",
    "        if self.mode =='training':\n",
    "            target = list(row[self.config.data_preparation.labels])\n",
    "            return image, target\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CropBorders:\n",
    "    def __init__(self, border_size):\n",
    "        self.border_size = border_size\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        img,\n",
    "        **kwargs\n",
    "    ):\n",
    "        height, width = img.shape[:2]\n",
    "        new_height = height - 2 * self.border_size\n",
    "        new_width = width - 2 * self.border_size\n",
    "        cropped_img = img[\n",
    "                    self.border_size:height-self.border_size, \n",
    "                    self.border_size:width-self.border_size\n",
    "        ]\n",
    "        padded_img = np.zeros(\n",
    "            (height, width, img.shape[2]), \n",
    "            dtype=img.dtype\n",
    "        )\n",
    "        padded_img[\n",
    "            self.border_size:new_height+self.border_size,\n",
    "            self.border_size:new_width+self.border_size\n",
    "        ] = cropped_img\n",
    "        return padded_img\n",
    "    \n",
    "class Augmentation :\n",
    "    def __init__(self, config) :\n",
    "        self.input_shape = config.training_params.image_size\n",
    "        self.SEED_VAL  = config.seed\n",
    "\n",
    "    def seed_all(self):\n",
    "        random.seed(self.SEED_VAL)\n",
    "        np.random.seed(self.SEED_VAL)\n",
    "        torch.manual_seed(self.SEED_VAL)\n",
    "        torch.cuda.manual_seed_all(self.SEED_VAL)\n",
    "        os.environ['PYTHONHASHSEED'] = str(self.SEED_VAL)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    def train_transform(self, visualize=False) :\n",
    "        self.seed_all()\n",
    "        augs_list = [\n",
    "            A.Resize(height=self.input_shape[0], width=self.input_shape[1], p=1.0),\n",
    "            # Geometric augmentations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Transpose(p=0.5),  # Transpose the input with a probability of 0.5\n",
    "            A.RandomRotate90(p=0.5),  # Randomly rotate the input by 90 degrees with a probability of 0.5\n",
    "            # Blur/Noise augmentations\n",
    "            A.GaussianBlur(p=0.15),  # Apply Gaussian Blur to the input\n",
    "            A.GaussNoise(p=0.25),  # Apply Gaussian Noise to the input\n",
    "            \n",
    "        ]\n",
    "        if not visualize:\n",
    "                augs_list.extend(\n",
    "                 [\n",
    "                     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0),\n",
    "                     ToTensorV2(),\n",
    "                 ]   \n",
    "                )\n",
    "        train_transform = A.Compose(augs_list)\n",
    "        return train_transform\n",
    "\n",
    "    def test_trasnform(self, aug_type='Basic') :\n",
    "        if aug_type == 'Basic' :\n",
    "            test_transform = A.Compose([ \n",
    "                                      A.Resize(height=self.input_shape[0], width=self.input_shape[1], p=1),\n",
    "                                      A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0),\n",
    "                                      ToTensorV2(),\n",
    "                                      ])\n",
    "        elif aug_type == 'CropBorders' :\n",
    "            test_transform = A.Compose([ \n",
    "                                    #A.RandomRotate90(p=0.5),\n",
    "                                   # A.HorizontalFlip(p=0.5),\n",
    "                                    A.Resize(height=self.input_shape[0], width=self.input_shape[1], p=1.0),\n",
    "                                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0),\n",
    "                                    ToTensorV2(),\n",
    "                                      ])\n",
    "        return test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load an image from the disk.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB format\n",
    "    return image\n",
    "\n",
    "def augment_image(image, augmentation_pipeline):\n",
    "    \"\"\"Apply the augmentation pipeline to an image.\"\"\"\n",
    "    augmented = augmentation_pipeline(image=image)\n",
    "    return augmented['image']\n",
    "\n",
    "def display_augmented_images(\n",
    "    df,\n",
    "    augmentation_pipeline,\n",
    "    column_name,\n",
    "    num_images=25\n",
    "):\n",
    "    \"\"\"Display a 5x5 grid of augmented images from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Dataframe containing images or paths to images.\n",
    "    column_name (str): The column name where image paths or images are stored.\n",
    "    num_images (int): Total number of images to display (default 25, should be a perfect square for a grid).\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(15, 15))\n",
    "    fig.suptitle('5x5 Grid of Augmented Images')\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            set_seed(42 * (i+1)*(j+1))\n",
    "            idx = np.random.randint(0, len(df))  # Randomly select an image from the dataframe\n",
    "            image_data = df.iloc[idx][column_name]\n",
    "\n",
    "            # Load image if the dataframe contains paths\n",
    "            if isinstance(image_data, str):\n",
    "                image = load_image(image_data)\n",
    "            else:\n",
    "                image = image_data  # Assume it's an array\n",
    "\n",
    "            augmented_image = augment_image(image, augmentation_pipeline)\n",
    "            axs[i, j].imshow(augmented_image)\n",
    "            axs[i, j].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CountModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for training and validation of a model for palm tree counting.\n",
    "    \n",
    "    Parameters:\n",
    "    lr (float): Learning rate for the optimizer.\n",
    "    weight_decay (float, optional): Weight decay for the optimizer. Default is 1e-4.\n",
    "    warmup_steps (int, optional): Number of warmup steps for the learning rate scheduler. Default is 0.\n",
    "    \"\"\" \n",
    "    def __init__(self, lr, weight_decay=1e-4, warmup_steps=0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = HouseCounter(CFG).to(CFG.device)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.val_accuracy = torchmetrics.MeanAbsoluteError()\n",
    "        self.train_accuracy = torchmetrics.MeanAbsoluteError()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step for a single batch.\n",
    "        \n",
    "        Parameters:\n",
    "        batch (tuple): Batch of data containing inputs and targets.\n",
    "        batch_idx (int): Index of the batch.\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The loss value for the batch.\n",
    "        \"\"\"\n",
    "        x, targets = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.criterion(out, torch.stack(targets, dim=1).float())\n",
    "        self.log('loss', loss)\n",
    "        self.train_accuracy.update(out, torch.stack(targets, dim=1))\n",
    "        optimizer = self.optimizers()\n",
    "        self.log('lr', optimizer.param_groups[0]['lr'], prog_bar=True)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step for a single batch.\n",
    "        \n",
    "        Parameters:\n",
    "        batch (tuple): Batch of data containing inputs and targets.\n",
    "        batch_idx (int): Index of the batch.\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The loss value for the batch.\n",
    "        \"\"\"\n",
    "        x, targets = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.criterion(out, torch.stack(targets, dim=1).float())\n",
    "        self.val_accuracy.update(out, torch.stack(targets, dim=1))\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Actions to perform at the end of a validation epoch.\n",
    "        \n",
    "        Logs the Mean Absolute Error (MAE) for training and validation.\n",
    "        Resets the accuracy metrics.\n",
    "        \"\"\"\n",
    "        train_mae = self.train_accuracy.compute()\n",
    "        self.log('train_mae', train_mae, prog_bar=True)\n",
    "        self.train_accuracy.reset()\n",
    "        # Compute MAE for all targets\n",
    "        val_mae = self.val_accuracy.compute()\n",
    "        self.log('val_mae', val_mae, prog_bar=True)\n",
    "        self.val_accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer and learning rate scheduler.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: The optimizer and the learning rate scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.lr, weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, self.hparams.warmup_steps, self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention mechanism for capturing long-range dependencies in image data.\n",
    "    \n",
    "    Parameters:\n",
    "    in_channels (int): Number of input channels.\n",
    "    key_channels (int, optional): Number of channels for the key and query. Defaults to in_channels // 8.\n",
    "    value_channels (int, optional): Number of channels for the value. Defaults to in_channels // 2.\n",
    "    out_channels (int, optional): Number of output channels. Defaults to in_channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, key_channels=None, value_channels=None, out_channels=None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Set the number of channels for key, value, and output\n",
    "        self.key_channels = key_channels if key_channels is not None else in_channels // 8\n",
    "        self.value_channels = value_channels if value_channels is not None else in_channels // 2\n",
    "        self.out_channels = out_channels if out_channels is not None else in_channels\n",
    "\n",
    "        # Define convolutional layers for query, key, and value\n",
    "        self.query_conv = nn.Conv2d(in_channels, self.key_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, self.key_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, self.value_channels, kernel_size=1)\n",
    "        \n",
    "        # Learnable parameter for scaling the output\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Define convolutional layer for output, followed by batch normalization and ReLU activation\n",
    "        self.out_conv = nn.Conv2d(self.value_channels, self.out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Self-Attention mechanism.\n",
    "        \n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor of the same shape as the input tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract batch size, number of channels, height, and width from the input tensor\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Apply query, key, and value convolutions and reshape the tensors\n",
    "        query = self.query_conv(x).view(batch_size, self.key_channels, -1)\n",
    "        key = self.key_conv(x).view(batch_size, self.key_channels, -1)\n",
    "        value = self.value_conv(x).view(batch_size, self.value_channels, -1)\n",
    "        \n",
    "        # Compute the energy (attention scores) by matrix multiplying query and key\n",
    "        energy = torch.bmm(query.transpose(1, 2), key)\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        \n",
    "        # Compute the output by matrix multiplying value and attention weights\n",
    "        out = torch.bmm(value, attention.transpose(1, 2))\n",
    "        \n",
    "        # Reshape the output to its original dimensions\n",
    "        out = out.view(batch_size, self.value_channels, height, width)\n",
    "        \n",
    "        # Apply output convolution, batch normalization, and ReLU activation\n",
    "        out = self.out_conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Scale the output by gamma and add the input tensor to get the final output\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class HouseCounter(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for counting houses in images using a pre-trained backbone\n",
    "    and self-attention mechanisms.\n",
    "    \n",
    "    Parameters:\n",
    "    config (object): Configuration object containing training parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(HouseCounter, self).__init__()\n",
    "        \n",
    "        # Create the pre-trained model using timm library\n",
    "        self.model_name = config.training_params.model_name\n",
    "        self.model = timm.create_model(\n",
    "            self.model_name,\n",
    "            pretrained=True\n",
    "        )\n",
    "        \n",
    "        # Set the number of input channels for the attention layers\n",
    "        in_channels = config.training_params.attention_in_channels\n",
    "        \n",
    "        # Define two self-attention layers\n",
    "        self.attention1 = SelfAttention(in_channels=in_channels)\n",
    "        self.attention2 = SelfAttention(in_channels=in_channels)\n",
    "        \n",
    "        # Define a fully connected layer for the final output\n",
    "        self.fc = nn.Linear(in_channels, 3)\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward pass of the HouseCounter model.\n",
    "        \n",
    "        Parameters:\n",
    "        image (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor with the counts of palm trees in three categories.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract features from the image using the pre-trained model\n",
    "        x = self.model.forward_features(image)\n",
    "        \n",
    "        # Apply the first self-attention layer\n",
    "        x = self.attention1(x)\n",
    "        \n",
    "        # Apply the second self-attention layer\n",
    "        x = self.attention2(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        \n",
    "        # Pass the pooled features through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_data(\n",
    "    dirpath,\n",
    "    df,\n",
    "    folds,\n",
    "    image_path,\n",
    "    aug,\n",
    "    seed,\n",
    "    mode='valid',\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to predict on data using trained models across specified folds.\n",
    "    \n",
    "    Parameters:\n",
    "    dirpath (str): Directory path where the model checkpoints are stored.\n",
    "    df (pd.DataFrame): DataFrame containing the data and metadata for prediction.\n",
    "    folds (list): List of fold indices to perform predictions on.\n",
    "    image_path (str): Path to the directory containing images.\n",
    "    aug (function): Augmentation function to apply to the images.\n",
    "    seed (int): Seed for random operations to ensure reproducibility.\n",
    "    mode (str): Mode of prediction, either 'valid' or 'test'. Default is 'valid'.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - all_predictions (list): List of all predictions made.\n",
    "        - background_data_select_all (pd.DataFrame): DataFrame of selected background data for validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_predictions = []\n",
    "    background_data_select_all=pd.DataFrame()\n",
    "    for fold in folds:\n",
    "        print(f'--------------------------------Prediction Fold {fold+1}/5---------------------------------')\n",
    "        if mode == 'valid':\n",
    "            random.seed(seed*fold)\n",
    "            predict_data = df[df['fold'] == fold].reset_index(drop=True)\n",
    "            background_data_in_df = df[df['count'] == 0]['image_id'].values.tolist()\n",
    "            background_data_in_val = predict_data[predict_data['count'] == 0]['image_id'].values.tolist()\n",
    "            background_data_select_from = background_data[~background_data['image_id'].isin(background_data_in_df)]['image_id'].values.tolist()\n",
    "            number_background_data_add = predict_data.shape[0] - 2*len(background_data_in_val)\n",
    "            image_id_background_data_select = random.choices(background_data_select_from, k=number_background_data_add)\n",
    "            background_data_select = background_data[background_data['image_id'].isin(image_id_background_data_select)]\n",
    "            predict_data = pd.concat([predict_data, background_data_select], axis=0).reset_index(drop=True)\n",
    "            background_data_select['fold']=fold\n",
    "            background_data_select_all=pd.concat([background_data_select_all,background_data_select]).reset_index(drop=True)\n",
    "        else:\n",
    "            predict_data = df.copy().reset_index(drop=True)\n",
    "            \n",
    "        test_dataset = ImageDataset(\n",
    "            predict_data,\n",
    "            CFG,\n",
    "            aug,\n",
    "            mode='training' if mode == 'valid' else 'testing'\n",
    "        )\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=CFG.training_params.valid_bs,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.training_params.num_workers\n",
    "        )\n",
    "    \n",
    "        checkpoint_path = os.path.join(\n",
    "            dirpath,\n",
    "            f'best_model_fold_{fold+1}.ckpt' \n",
    "        )\n",
    "        \n",
    "        count_model = CountModel.load_from_checkpoint(checkpoint_path)\n",
    "        if torch.cuda.is_available():\n",
    "            count_model.cuda()\n",
    "        count_model.freeze()\n",
    "        count_model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        for batch in tqdm(data_loader):\n",
    "            if mode == 'valid':\n",
    "                x = batch[0].cuda()\n",
    "            else:\n",
    "                x = batch.cuda()\n",
    "            out = count_model.model(x)\n",
    "            preds = [torch.relu(output).cpu().numpy() for output in out] \n",
    "            all_preds.extend(preds) \n",
    "        if mode == 'valid':\n",
    "            all_predictions.extend(all_preds)\n",
    "        else:\n",
    "            all_predictions.append(all_preds)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return all_predictions,background_data_select_all\n",
    "\n",
    "def generate_validation_report(\n",
    "    df,\n",
    "    folds,\n",
    "    model_prediction,\n",
    "    background_data_select,\n",
    "    classes_columns = ['category_0', 'category_1', 'category_2']\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates a validation report with Mean Absolute Error (MAE) scores for model predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the ground truth data.\n",
    "    folds (list): List of fold indices used in validation.\n",
    "    model_prediction (list): List of model predictions.\n",
    "    background_data_select (pd.DataFrame): DataFrame containing selected background data.\n",
    "    classes_columns (list, optional): List of column names representing the target categories. Default is ['category_0', 'category_1', 'category_2'].\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with MAE scores for each category and a flattened category.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_gt = pd.DataFrame()\n",
    "    for fold in folds:\n",
    "        df_with_back=pd.concat([df[df.fold==fold].reset_index(drop=True),\n",
    "                                 background_data_select[background_data_select.fold==fold].reset_index(drop=True)]\n",
    "                              ).reset_index(drop=True)\n",
    "        df_gt = pd.concat(\n",
    "            [\n",
    "                df_gt,\n",
    "                df_with_back\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    df_predictions = pd.DataFrame(\n",
    "        model_prediction,\n",
    "        columns=classes_columns\n",
    "    )\n",
    "    \n",
    "    # Calculating MAE for each target\n",
    "    mae_scores = {col: mean_absolute_error(df_gt[col], df_predictions[col]) for col in classes_columns}\n",
    "    gt = df_gt[classes_columns].values.reshape(-1, 1)\n",
    "    pred = np.array(model_prediction).reshape(-1, 1)\n",
    "    flatten_mae = mean_absolute_error(gt, pred)\n",
    "    mae_scores['flatten_category'] = flatten_mae\n",
    "    return mae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "def free_memory(sleep_time=0.1) :\n",
    "    \n",
    "    \"\"\"\n",
    "    This function helps to free up memory by:\n",
    "    1. Forcing garbage collection.\n",
    "    2. Synchronizing CUDA operations.\n",
    "    3. Emptying the CUDA cache.\n",
    "    4. Introducing a sleep delay to ensure proper memory release.\n",
    "    \n",
    "    Parameters:\n",
    "    sleep_time (float): Duration to pause execution after memory cleanup. Default is 0.1 seconds.\n",
    "    \"\"\"    \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "free_memory(sleep_time=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    device = 'cuda'\n",
    "    class data_preparation:\n",
    "        images_path = \"kedro-zindi/data/dataset/Images/Images\"\n",
    "        train_path = \"kedro-zindi/data/dataset/Train.csv\"\n",
    "        test_path = \"kedro-zindi/data/dataset/Test.csv\"\n",
    "        ss_path = \"kedro-zindi/data/dataset/SampleSubmission.csv\"\n",
    "        method = 'groupkfold'\n",
    "        targets = [1, 2, 3]\n",
    "        labels = ['category_0', 'category_1', 'category_2']\n",
    "        group_col = 'image_id'\n",
    "        split_col = 'category_id'\n",
    "        n_splits = 5\n",
    "        downsample_factor = 0.1\n",
    "        \n",
    "    class training_params:\n",
    "        image_path_col = \"path\"\n",
    "        model_name = 'convnext_base'\n",
    "        attention_in_channels = 1024\n",
    "        num_classes = 3\n",
    "        epochs = 25\n",
    "        image_size = [512,512]\n",
    "        num_workers =6\n",
    "        train_bs = 8\n",
    "        valid_bs = 32\n",
    "        criterion = \"l1loss\"\n",
    "        lr = 1e-4\n",
    "        weight_decay = 1e-4\n",
    "        warmup_steps = 0\n",
    "        class model_checkpoint:\n",
    "            monitor = 'val_loss'  \n",
    "            mode = 'min'          \n",
    "            dirpath = 'dataset/multilabelregression/'\n",
    "            filename = 'best_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.data_preparation.train_path)\n",
    "test = pd.read_csv(CFG.data_preparation.test_path)\n",
    "ss = pd.read_csv(CFG.data_preparation.ss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c1fcf27fd84812b93098b856c9e2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>count</th>\n",
       "      <th>fold</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00swku56d3h0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_00sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00w4th51hio0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_00w4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_015i4fpen1ws</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_015i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id_01gz1pz8e9w6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_01gz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id_02616gseocdo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_0261...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  category_0  category_1  category_2  count  fold  \\\n",
       "0  id_00swku56d3h0           0           1           0      1     3   \n",
       "1  id_00w4th51hio0           0           2           1      3     1   \n",
       "4  id_015i4fpen1ws           0           1           0      1     2   \n",
       "5  id_01gz1pz8e9w6           0           2           0      2     2   \n",
       "7  id_02616gseocdo           0           1           0      1     1   \n",
       "\n",
       "                                                path  \n",
       "0  kedro-zindi/data/dataset/Images/Images/id_00sw...  \n",
       "1  kedro-zindi/data/dataset/Images/Images/id_00w4...  \n",
       "4  kedro-zindi/data/dataset/Images/Images/id_015i...  \n",
       "5  kedro-zindi/data/dataset/Images/Images/id_01gz...  \n",
       "7  kedro-zindi/data/dataset/Images/Images/id_0261...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train, train_data =  create_target_df(df=train)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data[['image_id', 'fold']].to_csv('common_split_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b44963f680e4d5a800d23ad3a1b9bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering (2307, 8)\n",
      "after filtering (1700, 12)\n"
     ]
    }
   ],
   "source": [
    "# oversampling utlities\n",
    "\n",
    "# get background data\n",
    "background_data = original_train[~(original_train['category_id'].isin(CFG.data_preparation.targets))].reset_index(drop=True)\n",
    "annotated_data = original_train[(original_train['category_id'].isin(CFG.data_preparation.targets))].reset_index(drop=True)\n",
    "\n",
    "# Use joblib to process paths in parallel and get results\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(bad_background)(path) for path in tqdm(background_data.path.tolist())\n",
    ")\n",
    "background_data['bad_background'] = results\n",
    "print('before filtering', background_data.shape)\n",
    "background_data = background_data[background_data['bad_background']==False].reset_index(drop=True)\n",
    "background_data[['category_0','category_1','category_2','count']]=0\n",
    "print('after filtering', background_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "background_data.to_csv('background_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['path'] = test['image_id'].apply(\n",
    "    lambda x : os.path.join(\n",
    "        CFG.data_preparation.images_path,\n",
    "        f\"{x}.tif\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>count</th>\n",
       "      <th>fold</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00swku56d3h0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_00sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00w4th51hio0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_00w4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_015i4fpen1ws</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_015i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id_01gz1pz8e9w6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_01gz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id_02616gseocdo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_0261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24075</th>\n",
       "      <td>id_ign1d6akc9pn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_ign1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24076</th>\n",
       "      <td>id_8rpbcywu54ys</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_8rpb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24077</th>\n",
       "      <td>id_6hhdjia8fcmd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_6hhd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24078</th>\n",
       "      <td>id_4mqlt5ociz1y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_4mql...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24079</th>\n",
       "      <td>id_nvn39iuxu6ac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>kedro-zindi/data/dataset/Images/Images/id_nvn3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2696 rows Ã 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_id  category_0  category_1  category_2  count  fold  \\\n",
       "0      id_00swku56d3h0           0           1           0      1     3   \n",
       "1      id_00w4th51hio0           0           2           1      3     1   \n",
       "4      id_015i4fpen1ws           0           1           0      1     2   \n",
       "5      id_01gz1pz8e9w6           0           2           0      2     2   \n",
       "7      id_02616gseocdo           0           1           0      1     1   \n",
       "...                ...         ...         ...         ...    ...   ...   \n",
       "24075  id_ign1d6akc9pn           0           0           0      0     2   \n",
       "24076  id_8rpbcywu54ys           0           0           0      0     0   \n",
       "24077  id_6hhdjia8fcmd           0           0           0      0     2   \n",
       "24078  id_4mqlt5ociz1y           0           0           0      0     1   \n",
       "24079  id_nvn39iuxu6ac           0           0           0      0     3   \n",
       "\n",
       "                                                    path  \n",
       "0      kedro-zindi/data/dataset/Images/Images/id_00sw...  \n",
       "1      kedro-zindi/data/dataset/Images/Images/id_00w4...  \n",
       "4      kedro-zindi/data/dataset/Images/Images/id_015i...  \n",
       "5      kedro-zindi/data/dataset/Images/Images/id_01gz...  \n",
       "7      kedro-zindi/data/dataset/Images/Images/id_0261...  \n",
       "...                                                  ...  \n",
       "24075  kedro-zindi/data/dataset/Images/Images/id_ign1...  \n",
       "24076  kedro-zindi/data/dataset/Images/Images/id_8rpb...  \n",
       "24077  kedro-zindi/data/dataset/Images/Images/id_6hhd...  \n",
       "24078  kedro-zindi/data/dataset/Images/Images/id_4mql...  \n",
       "24079  kedro-zindi/data/dataset/Images/Images/id_nvn3...  \n",
       "\n",
       "[2696 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.to_csv('train_data.csv', index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Training Fold 1/5---------------------------------\n",
      "NÂ° Images before oversampling 2160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9f397a4dc742eba19c08121e82667c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b283a9b130a849a7bbec98bad08783bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e044240bba5419d9834b07b454bc338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÂ° Images after oversampling 4089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type              | Params\n",
      "-----------------------------------------------------\n",
      "0 | model          | HouseCounter      | 91.2 M\n",
      "1 | criterion      | L1Loss            | 0     \n",
      "2 | val_accuracy   | MeanAbsoluteError | 0     \n",
      "3 | train_accuracy | MeanAbsoluteError | 0     \n",
      "-----------------------------------------------------\n",
      "91.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "91.2 M    Total params\n",
      "364.895   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c95bd875b484e5b84d44e8d113a90af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Generate a timestamp for directory naming\n",
    "timestamp = datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "\n",
    "# Define directory path for saving model checkpoints\n",
    "dirpath = f'{CFG.training_params.model_checkpoint.dirpath}{CFG.training_params.model_name}/{timestamp}'\n",
    "\n",
    "# Define path for CopyPaste augmentation images\n",
    "folder_path_copypaste = \"dataset/Images/CopyPasteAugment\"\n",
    "\n",
    "# Initialize a list to store all predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Loop through each fold for cross-validation\n",
    "for fold in range(1):\n",
    "    # Remove the CopyPaste augmentation folder if it exists and create a new one\n",
    "    !rm -r $folder_path_copypaste\n",
    "    os.makedirs(folder_path_copypaste, exist_ok=True)\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(42)\n",
    "    print(f'--------------------------------Training Fold {fold+1}/5---------------------------------')\n",
    "    \n",
    "    # Prepare the training fold data\n",
    "    train_fold = train_data[train_data.fold!=fold].reset_index(drop=True)\n",
    "    \n",
    "    # Select annotated data for the current fold\n",
    "    annotated_data_fold = annotated_data[\n",
    "        annotated_data['path'].isin(train_fold.path.unique())\n",
    "    ].reset_index(drop=True)\n",
    "    print(\"NÂ° Images before oversampling\", len(train_fold))\n",
    "    \n",
    "    # Seed for augmentation processes\n",
    "    seed = (fold+1) * int(len(annotated_data_fold) // 42)\n",
    "    \n",
    "    \n",
    "    # Create augmented data using different techniques\n",
    "    scaled_masked_data = masked_scale(\n",
    "        annotated_data_fold,\n",
    "        output_directory=\"dataset/Images/Masked_and_Scaled\",\n",
    "        seed=seed\n",
    "    )\n",
    "    random_masked_data = random_masking(\n",
    "        labels_data=annotated_data_fold,\n",
    "        columns=annotated_data_fold.columns,\n",
    "        output_directory='dataset/Images/RandomMasking',\n",
    "        seed=seed\n",
    "    )\n",
    "    copypaste_data = copy_paste(\n",
    "        labels_data=annotated_data_fold,\n",
    "        background_data=background_data,\n",
    "        folder_path=folder_path_copypaste,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Count occurrences of each category per image_id in the copypaste data\n",
    "    category_counts = copypaste_data.groupby(['image_id', 'category_id']).size().unstack(fill_value=0)\n",
    "    \n",
    "\n",
    "    # Rename columns for categories\n",
    "    category_counts.columns = [f'category_{int(x-1)}' for x in category_counts.columns]\n",
    "    \n",
    "    # Add missing columns\n",
    "    for col in CFG.data_preparation.labels:\n",
    "        if col not in category_counts.columns:\n",
    "            category_counts[col] = 0\n",
    "            \n",
    "            \n",
    "    # Merge category counts with copypaste data\n",
    "    result_data = copypaste_data.merge(category_counts, on='image_id', how='left').copy()\n",
    "    result_data['count'] = 0\n",
    "    result_data['fold'] = 0\n",
    "    result_data = result_data.drop_duplicates('image_id')\n",
    "    \n",
    "    # Concatenate different augmented data to the training fold\n",
    "    train_fold = pd.concat(\n",
    "        [\n",
    "            train_fold,\n",
    "            result_data[train_fold.columns]\n",
    "        ]\n",
    "    ).reset_index(drop=True)\n",
    "    train_fold = pd.concat(\n",
    "        [\n",
    "            train_fold,\n",
    "            random_masked_data[train_fold.columns]\n",
    "        ]\n",
    "    ).reset_index(drop=True)\n",
    "    train_fold = pd.concat(\n",
    "        [\n",
    "            train_fold,\n",
    "            scaled_masked_data[train_fold.columns]\n",
    "        ]\n",
    "    ).reset_index(drop=True).sample(frac=1.0, random_state=seed)\n",
    "    print(\"NÂ° Images after oversampling\", len(train_fold))\n",
    "\n",
    "    # Prepare the validation fold data\n",
    "    val_fold = train_data[train_data.fold==fold].reset_index()\n",
    "    \n",
    "    # Define data augmentation techniques for training and validation\n",
    "    train_aug = Augmentation(CFG).train_transform(visualize=False)\n",
    "    test_aug = Augmentation(CFG).test_trasnform()\n",
    "    \n",
    "    # Create datasets and data loaders for training and validation\n",
    "    train_dataset = ImageDataset(train_fold, CFG, train_aug, mode='training')\n",
    "    valid_dataset = ImageDataset(val_fold, CFG, test_aug, mode='training')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.training_params.train_bs,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=1,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=CFG.training_params.valid_bs,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.training_params.num_workers,\n",
    "    )\n",
    "    \n",
    "    # Initialize the model and training setup\n",
    "    model = HouseCounter(CFG).to(device)\n",
    "    count_model = CountModel(CFG.training_params.lr)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',  \n",
    "        mode='min',          \n",
    "        dirpath=dirpath,\n",
    "        filename=f'best_model_fold_{fold+1}' \n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        max_epochs=CFG.training_params.epochs,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_mae\", mode=\"min\",patience=8),checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(count_model, train_loader, val_loader)\n",
    "    \n",
    "    # Clean up and free memory after each fold\n",
    "    del model, trainer, count_model\n",
    "    free_memory(sleep_time=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 1/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d58070a09e4f369177f45797f73b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 2/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32b06e92e1a47a99f51c4ff4936c95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 3/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0e1e30d2cc4c9eb03d0f3039760792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 4/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e72b7ca61bd46d48ced7fdbfacfe901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 5/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c336ff7d1c964f01a8ea9d699966895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_results,background_data_select = predict_on_data(\n",
    "    dirpath=dirpath,\n",
    "    df=train_data,\n",
    "    folds=range(5),\n",
    "    image_path='dataset/Images/',\n",
    "    aug=Augmentation(CFG).test_trasnform(aug_type='Basic'),\n",
    "    seed=CFG.seed,\n",
    "    mode='valid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = generate_validation_report(\n",
    "    df=train_data,\n",
    "    folds=range(5),\n",
    "    model_prediction=valid_results,\n",
    "    background_data_select=background_data_select,\n",
    "    classes_columns = ['category_0', 'category_1', 'category_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category_0': 0.039431319979209734,\n",
       " 'category_1': 0.6369141828670838,\n",
       " 'category_2': 0.2690324473633401,\n",
       " 'flatten_category': 0.3151259834032112}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 1/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0238441ed8a7487ab0cbedcf9172134b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 2/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f977dcae442c4202be57fda0eeb69000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 3/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063463eec51249dc8d73217d911da52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 4/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e5e277b51f4063bddff22b2eb4c76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------Prediction Fold 5/5---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec7236df6fe491ba5f943e9d209b22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results,_ = predict_on_data(\n",
    "    dirpath=dirpath,\n",
    "    df=test,\n",
    "    folds=range(5),\n",
    "    image_path='dataset/Images/',\n",
    "    aug=Augmentation(CFG).test_trasnform(aug_type='Basic'),\n",
    "    seed=CFG.seed,\n",
    "    mode='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2045, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = np.mean(test_results, axis=0)\n",
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = final_pred.reshape(-1,1)\n",
    "ss[\"Target\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_uawi0k5spci5_1</td>\n",
       "      <td>0.001877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_uawi0k5spci5_2</td>\n",
       "      <td>1.015339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_uawi0k5spci5_3</td>\n",
       "      <td>0.002165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_ba566jv4xzln_1</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_ba566jv4xzln_2</td>\n",
       "      <td>2.015723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6130</th>\n",
       "      <td>id_1z2r03cy53rx_2</td>\n",
       "      <td>11.119214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>id_1z2r03cy53rx_3</td>\n",
       "      <td>0.027630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6132</th>\n",
       "      <td>id_jg3tv9d3whai_1</td>\n",
       "      <td>0.004671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133</th>\n",
       "      <td>id_jg3tv9d3whai_2</td>\n",
       "      <td>9.708788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>id_jg3tv9d3whai_3</td>\n",
       "      <td>0.044718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6135 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image_id     Target\n",
       "0     id_uawi0k5spci5_1   0.001877\n",
       "1     id_uawi0k5spci5_2   1.015339\n",
       "2     id_uawi0k5spci5_3   0.002165\n",
       "3     id_ba566jv4xzln_1   0.001711\n",
       "4     id_ba566jv4xzln_2   2.015723\n",
       "...                 ...        ...\n",
       "6130  id_1z2r03cy53rx_2  11.119214\n",
       "6131  id_1z2r03cy53rx_3   0.027630\n",
       "6132  id_jg3tv9d3whai_1   0.004671\n",
       "6133  id_jg3tv9d3whai_2   9.708788\n",
       "6134  id_jg3tv9d3whai_3   0.044718\n",
       "\n",
       "[6135 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss.to_csv(\"Convnext_Float.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4604308,
     "sourceId": 7851211,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4605322,
     "sourceId": 7852537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
